{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60e6be53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10d1c690",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = pd.read_csv('./data.csv')\n",
    "df_raw.rename(columns = {'Sentence':'sentence', 'Sentiment':'sentiment'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15a0aef4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The GeoSolutions technology will leverage Bene...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>$ESI on lows, down $1.50 to $2.50 BK a real po...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>For the last quarter of 2010 , Componenta 's n...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>According to the Finnish-Russian Chamber of Co...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Swedish buyout firm has sold its remaining...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence sentiment\n",
       "0  The GeoSolutions technology will leverage Bene...  positive\n",
       "1  $ESI on lows, down $1.50 to $2.50 BK a real po...  negative\n",
       "2  For the last quarter of 2010 , Componenta 's n...  positive\n",
       "3  According to the Finnish-Russian Chamber of Co...   neutral\n",
       "4  The Swedish buyout firm has sold its remaining...   neutral"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8faaf0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5842\n"
     ]
    }
   ],
   "source": [
    "print(len(df_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4284cfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'negative', 'neutral', 'positive'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw.sentiment.values\n",
    "set(df_raw.sentiment.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b657c0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_raw[df_raw.sentiment != \"neutral\"] # only take rows that does not have 'neutral' in the choose_one column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d004f01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2712, 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bfaaf765",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_value = dict({\n",
    "                    'positive':1,\n",
    "                    'negative':0\n",
    "                  })\n",
    "#Dictionary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "616cb0d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-6633665d2415>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['value'] = df.sentiment.map(sentiment_value)\n"
     ]
    }
   ],
   "source": [
    "df['value'] = df.sentiment.map(sentiment_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1b092a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The GeoSolutions technology will leverage Bene...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>$ESI on lows, down $1.50 to $2.50 BK a real po...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>For the last quarter of 2010 , Componenta 's n...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>$SPY wouldn't be surprised to see a green close</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Shell's $70 Billion BG Deal Meets Shareholder ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence sentiment  value\n",
       "0  The GeoSolutions technology will leverage Bene...  positive      1\n",
       "1  $ESI on lows, down $1.50 to $2.50 BK a real po...  negative      0\n",
       "2  For the last quarter of 2010 , Componenta 's n...  positive      1\n",
       "5    $SPY wouldn't be surprised to see a green close  positive      1\n",
       "6  Shell's $70 Billion BG Deal Meets Shareholder ...  negative      0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3aff65d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making of hash map for bag of words /term frequency(tf)\n",
    "\n",
    "def make_hash_map(df): #Main function\n",
    "    hash_map = {}\n",
    "    for index, row in df.iterrows():\n",
    "        hash_map = map_book(hash_map, extract_words(row['sentence']))\n",
    "    return hash_map\n",
    "\n",
    "#This is a function that has been defined for tokenization of words\n",
    "def extract_words(sentence):\n",
    "    '''This is to clean and tokenize words'''\n",
    "    ignore_words = ['a', 'the', 'if', 'br', 'and', 'of', 'to', 'is', 'are', 'he', 'she', 'my', 'you', 'it','how']\n",
    "    words = re.sub(\"[^\\w]\", \" \",  sentence).split() # this replaces all special chars with ' '\n",
    "    words = [word.lower() for word in words]\n",
    "    words_cleaned = [w.lower() for w in words if w not in ignore_words]\n",
    "    return words_cleaned \n",
    "\n",
    "def map_book(hash_map, tokens):\n",
    "    if tokens is not None:\n",
    "        for word in tokens:\n",
    "            # Word Exist?\n",
    "            if word in hash_map:\n",
    "                hash_map[word] = hash_map[word] + 1\n",
    "            else:\n",
    "                hash_map[word] = 1\n",
    "\n",
    "        return hash_map\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e21aa3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function frequent_vocab with the following input: word_freq and max_features\n",
    "#Not understood !!\n",
    "def frequent_vocab(word_freq, max_features): \n",
    "    counter = 0  #initialize counter with the value zero\n",
    "    vocab = []   # create an empty list called vocab\n",
    "    # list words in the dictionary in descending order of frequency\n",
    "    for key, value in sorted(word_freq.items(), key=lambda item: (item[1], item[0]), reverse=True): \n",
    "       #loop function to get the top (max_features) number of words\n",
    "        if counter<max_features: \n",
    "            vocab.append(key)\n",
    "            counter+=1\n",
    "        else: break\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4832de1c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hash_map = make_hash_map(df)\n",
    "#experiment with vocab part 500 not fixed\n",
    "vocab=frequent_vocab(hash_map, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dcbddd38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['in',\n",
       " 'eur',\n",
       " 'for',\n",
       " 'from',\n",
       " 's',\n",
       " 'mn',\n",
       " 'on',\n",
       " 'profit',\n",
       " '1',\n",
       " 'sales',\n",
       " 'year',\n",
       " 'its',\n",
       " 'by',\n",
       " 'net',\n",
       " 'company',\n",
       " '2',\n",
       " 'finnish',\n",
       " 'co',\n",
       " 'with',\n",
       " 'said',\n",
       " 'has',\n",
       " '3',\n",
       " 'million',\n",
       " '5',\n",
       " 'as',\n",
       " 'will',\n",
       " 'm',\n",
       " 'period',\n",
       " 'at',\n",
       " '4',\n",
       " 'up',\n",
       " 'operating',\n",
       " 'quarter',\n",
       " '0',\n",
       " 'mln',\n",
       " '6',\n",
       " 'that',\n",
       " '2009',\n",
       " '7',\n",
       " 'was',\n",
       " '8',\n",
       " '2008',\n",
       " '9',\n",
       " 't',\n",
       " 'be',\n",
       " 'loss',\n",
       " '2007',\n",
       " 'this',\n",
       " 'first',\n",
       " '2010',\n",
       " 'http',\n",
       " 'share',\n",
       " 'compared',\n",
       " 'oyj',\n",
       " 'group',\n",
       " 'stks',\n",
       " 'https',\n",
       " 'down',\n",
       " 'today',\n",
       " 'an',\n",
       " 'new',\n",
       " 'corresponding',\n",
       " 'finland',\n",
       " 'euro',\n",
       " 'market',\n",
       " 'increased',\n",
       " 'rose',\n",
       " 'percent',\n",
       " '2006',\n",
       " 'long',\n",
       " 'have',\n",
       " '000',\n",
       " 'aapl',\n",
       " '10',\n",
       " 'price',\n",
       " 'also',\n",
       " 'after',\n",
       " 'per',\n",
       " 'business',\n",
       " '2005',\n",
       " 'which',\n",
       " 'than',\n",
       " 'increase',\n",
       " 'been',\n",
       " 'we',\n",
       " 'i',\n",
       " 'hel',\n",
       " 'contract',\n",
       " 'our',\n",
       " 'last',\n",
       " 'buy',\n",
       " 'third',\n",
       " 'more',\n",
       " 'earlier',\n",
       " 'operations',\n",
       " 'growth',\n",
       " 'earnings',\n",
       " 'over',\n",
       " 'decreased',\n",
       " 'tsla',\n",
       " 'same',\n",
       " 'services',\n",
       " 'pct',\n",
       " 'helsinki',\n",
       " 'shares',\n",
       " 'second',\n",
       " 'now',\n",
       " 'result',\n",
       " 'stock',\n",
       " 'short',\n",
       " '12',\n",
       " 'eur0',\n",
       " 'billion',\n",
       " 'bank',\n",
       " 'about',\n",
       " '15',\n",
       " 'technology',\n",
       " 'some',\n",
       " 'off',\n",
       " 'maker',\n",
       " '11',\n",
       " 'out',\n",
       " '20',\n",
       " 'lower',\n",
       " 'agreement',\n",
       " 'order',\n",
       " 'higher',\n",
       " 'half',\n",
       " 'fell',\n",
       " 'all',\n",
       " '30',\n",
       " 'usd',\n",
       " 'solutions',\n",
       " 'plc',\n",
       " 'day',\n",
       " 'signed',\n",
       " 'nokia',\n",
       " 'months',\n",
       " 'financial',\n",
       " 'us',\n",
       " 'september',\n",
       " 'or',\n",
       " 'nine',\n",
       " 'expected',\n",
       " 'eps',\n",
       " '50',\n",
       " 'totalled',\n",
       " 'time',\n",
       " 'omx',\n",
       " 'good',\n",
       " 'corporation',\n",
       " 'products',\n",
       " 'month',\n",
       " 'into',\n",
       " 'high',\n",
       " 'here',\n",
       " 'spy',\n",
       " 'revenue',\n",
       " 'one',\n",
       " 'january',\n",
       " 'fb',\n",
       " 'deal',\n",
       " '18',\n",
       " '14',\n",
       " 'systems',\n",
       " 'companies',\n",
       " 'before',\n",
       " 'based',\n",
       " 'according',\n",
       " 'strong',\n",
       " 'news',\n",
       " 'back',\n",
       " 'won',\n",
       " 'supply',\n",
       " 'not',\n",
       " 'mobile',\n",
       " 'end',\n",
       " 'cost',\n",
       " 'above',\n",
       " '25',\n",
       " 'were',\n",
       " 'totaled',\n",
       " 'positive',\n",
       " 'line',\n",
       " 'ceo',\n",
       " 'term',\n",
       " 'support',\n",
       " 'had',\n",
       " 'grew',\n",
       " 'dividend',\n",
       " 'customers',\n",
       " 'both',\n",
       " 'while',\n",
       " 'very',\n",
       " 'service',\n",
       " 'would',\n",
       " 'well',\n",
       " 'two',\n",
       " 'total',\n",
       " 'steel',\n",
       " 'industry',\n",
       " 'close',\n",
       " 'cash',\n",
       " 'but',\n",
       " 'astrazeneca',\n",
       " 'their',\n",
       " 'says',\n",
       " 'paper',\n",
       " 'markets',\n",
       " 'can',\n",
       " 'awarded',\n",
       " '13',\n",
       " 'volume',\n",
       " 'still',\n",
       " 'russia',\n",
       " 'pretax',\n",
       " 'position',\n",
       " 'may',\n",
       " 'however',\n",
       " 'capital',\n",
       " '2004',\n",
       " '16',\n",
       " 'tesco',\n",
       " 'reports',\n",
       " 'production',\n",
       " 'acquisition',\n",
       " '23',\n",
       " '17',\n",
       " 'plant',\n",
       " 'next',\n",
       " 'looking',\n",
       " 'june',\n",
       " 'during',\n",
       " 'ago',\n",
       " '40',\n",
       " '21',\n",
       " 'unit',\n",
       " 'sell',\n",
       " 'rise',\n",
       " 'oil',\n",
       " 'non',\n",
       " 'ftse',\n",
       " 'expects',\n",
       " 'euros',\n",
       " 'china',\n",
       " '33',\n",
       " '19',\n",
       " '100',\n",
       " 'update',\n",
       " 'three',\n",
       " 'software',\n",
       " 'provider',\n",
       " 'october',\n",
       " 'march',\n",
       " 'items',\n",
       " 'demand',\n",
       " 'construction',\n",
       " 'chart',\n",
       " '35',\n",
       " 'week',\n",
       " 'see',\n",
       " 'manufacturer',\n",
       " 'management',\n",
       " 'global',\n",
       " 'estimated',\n",
       " 'due',\n",
       " 'target',\n",
       " 'take',\n",
       " 'significant',\n",
       " 'prices',\n",
       " 'previous',\n",
       " 'move',\n",
       " 'like',\n",
       " 'further',\n",
       " 'fourth',\n",
       " 'breakout',\n",
       " 'amounted',\n",
       " '31',\n",
       " 'x',\n",
       " 'under',\n",
       " 'just',\n",
       " 'investment',\n",
       " 'hit',\n",
       " 'fall',\n",
       " 'equipment',\n",
       " 'costs',\n",
       " 'between',\n",
       " 'adp',\n",
       " '70',\n",
       " '28',\n",
       " '2011',\n",
       " 'years',\n",
       " 'versus',\n",
       " 'they',\n",
       " 'reported',\n",
       " 'report',\n",
       " 'other',\n",
       " 'orders',\n",
       " 'model',\n",
       " 'excluding',\n",
       " 'bullish',\n",
       " 'announced',\n",
       " 'ab',\n",
       " 'went',\n",
       " 'uk',\n",
       " 'through',\n",
       " 'strategy',\n",
       " 'since',\n",
       " 'sabmiller',\n",
       " 'respectively',\n",
       " 'recurring',\n",
       " 'project',\n",
       " 'network',\n",
       " 'looks',\n",
       " 'largest',\n",
       " 'february',\n",
       " 'building',\n",
       " 'break',\n",
       " 'area',\n",
       " 'april',\n",
       " 'am',\n",
       " '700',\n",
       " '29',\n",
       " '22',\n",
       " 'value',\n",
       " 'swedish',\n",
       " 'so',\n",
       " 'sector',\n",
       " 'ruukki',\n",
       " 'resistance',\n",
       " 'real',\n",
       " 'plans',\n",
       " 'insurance',\n",
       " 'income',\n",
       " 'improved',\n",
       " 'flow',\n",
       " 'finnair',\n",
       " 'electronics',\n",
       " 'drug',\n",
       " 'current',\n",
       " 'countries',\n",
       " 'calls',\n",
       " 'around',\n",
       " 'again',\n",
       " '200',\n",
       " 'yit',\n",
       " 'trade',\n",
       " 'tesla',\n",
       " 'takeover',\n",
       " 'supplier',\n",
       " 'stocks',\n",
       " 'savings',\n",
       " 'posted',\n",
       " 'november',\n",
       " 'nice',\n",
       " 'negative',\n",
       " 'manufacturing',\n",
       " 'liters',\n",
       " 'head',\n",
       " 'eur1',\n",
       " 'estimates',\n",
       " 'cut',\n",
       " 'could',\n",
       " 'cooperation',\n",
       " 'august',\n",
       " '26',\n",
       " '24',\n",
       " 'yesterday',\n",
       " 'turnover',\n",
       " 'results',\n",
       " 'release',\n",
       " 'received',\n",
       " 'performance',\n",
       " 'people',\n",
       " 'part',\n",
       " 'offer',\n",
       " 'near',\n",
       " 'margin',\n",
       " 'make',\n",
       " 'low',\n",
       " 'life',\n",
       " 'index',\n",
       " 'handling',\n",
       " 'go',\n",
       " 'exchange',\n",
       " 'developer',\n",
       " 'chief',\n",
       " 'cent',\n",
       " 'barclays',\n",
       " 'baltic',\n",
       " '80',\n",
       " 'x20ac',\n",
       " 'via',\n",
       " 'stora',\n",
       " 'sale',\n",
       " 'run',\n",
       " 'q1',\n",
       " 'potential',\n",
       " 'number',\n",
       " 'narrowed',\n",
       " 'metal',\n",
       " 'level',\n",
       " 'july',\n",
       " 'interest',\n",
       " 'full',\n",
       " 'energy',\n",
       " 'double',\n",
       " 'director',\n",
       " 'data',\n",
       " 'daily',\n",
       " 'corp',\n",
       " 'continue',\n",
       " 'board',\n",
       " 'beer',\n",
       " 'annual',\n",
       " 'world',\n",
       " 'when',\n",
       " 'weak',\n",
       " 'use',\n",
       " 'upm',\n",
       " 'system',\n",
       " 'stake',\n",
       " 'staff',\n",
       " 'solution',\n",
       " 'small',\n",
       " 'retail',\n",
       " 'record',\n",
       " 'nflx',\n",
       " 'most',\n",
       " 'major',\n",
       " 'machinery',\n",
       " 'ltd',\n",
       " 'london',\n",
       " 'lead',\n",
       " 'improve',\n",
       " 'going',\n",
       " 'gains',\n",
       " 'forest',\n",
       " 'engineering',\n",
       " 'development',\n",
       " 'continuing',\n",
       " 'capacity',\n",
       " 'bid',\n",
       " 'best',\n",
       " 'america',\n",
       " 'added',\n",
       " '60',\n",
       " '45',\n",
       " '37',\n",
       " '36',\n",
       " '32',\n",
       " 'worth',\n",
       " 'traffic',\n",
       " 'thursday',\n",
       " 'taxes',\n",
       " 'sweden',\n",
       " 'standard',\n",
       " 'signal',\n",
       " 'shorts',\n",
       " 'scanfil',\n",
       " 'sbux',\n",
       " 'rt',\n",
       " 'remain',\n",
       " 'product',\n",
       " 'process',\n",
       " 'phone',\n",
       " 'oy',\n",
       " 'north',\n",
       " 'nordic',\n",
       " 'media',\n",
       " 'making',\n",
       " 'international',\n",
       " 'inc',\n",
       " 'inbev',\n",
       " 'fiscal',\n",
       " 'expand',\n",
       " 'enso',\n",
       " 'employees',\n",
       " 'elcoteq',\n",
       " 'december',\n",
       " 'continues',\n",
       " 'big',\n",
       " 'below',\n",
       " 'base',\n",
       " 'apple',\n",
       " 'another',\n",
       " 'analyst',\n",
       " '43',\n",
       " '42',\n",
       " 'working',\n",
       " 'whole',\n",
       " 'tuesday',\n",
       " 'trading',\n",
       " 'tax',\n",
       " 'should',\n",
       " 'shell',\n",
       " 'setup',\n",
       " 'ready',\n",
       " 'raised',\n",
       " 'points']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fca0d3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function bag of words with the following input: sentence and words\n",
    "def bagofwords(sentence, words):\n",
    "    sentence_words = extract_words(sentence) #tokenize sentences and assign it to variable sentence_words\n",
    "    # frequency word count\n",
    "    bag = np.zeros(len(words)) #create a NumPy array made up of zeroes with size len(words)\n",
    "    # loop through data and add value of 1 when token is present in the tweet\n",
    "    for sw in sentence_words:\n",
    "        for i,word in enumerate(words):\n",
    "            if word == sw: \n",
    "                bag[i] += 1\n",
    "                \n",
    "    return np.array(bag) # return the bag of word for one tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fc146910",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_words = len(vocab)\n",
    "no_documents = len(df)\n",
    "bag_ow = np.zeros([no_documents ,no_words])\n",
    "for i in range(no_documents) :\n",
    "    bag_ow[i ,:] = bagofwords(df['sentence'].iloc[i],vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ef998e86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2712, 500)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_ow.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5dc80eb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 2., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [1., 0., 1., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_ow"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6145c438",
   "metadata": {},
   "source": [
    "TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "99a38640",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('in', 1391),\n",
       " ('eur', 748),\n",
       " ('for', 619),\n",
       " ('from', 588),\n",
       " ('s', 499),\n",
       " ('mn', 459),\n",
       " ('on', 385),\n",
       " ('profit', 357),\n",
       " ('1', 322),\n",
       " ('sales', 321),\n",
       " ('year', 318),\n",
       " ('its', 305),\n",
       " ('by', 300),\n",
       " ('net', 297),\n",
       " ('company', 283),\n",
       " ('2', 275),\n",
       " ('finnish', 272),\n",
       " ('co', 268),\n",
       " ('with', 262),\n",
       " ('said', 258)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(hash_map.items(), key=lambda item: item[1], reverse=True)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cba3c561",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00000000e+000, 6.93745691e-310, 0.00000000e+000, 6.93745691e-310,\n",
       "       6.93745691e-310, 6.93744084e-310, 0.00000000e+000, 3.95252517e-323,\n",
       "       6.93744084e-310, 6.93745694e-310, 6.93745694e-310, 6.93745694e-310,\n",
       "       6.93745694e-310, 6.93745694e-310, 6.93745695e-310, 6.93745695e-310,\n",
       "       6.93745695e-310, 6.93745695e-310, 6.93745695e-310, 6.93745695e-310,\n",
       "       6.93745695e-310, 6.93745695e-310, 6.93745695e-310, 6.93745695e-310,\n",
       "       6.93745695e-310, 6.93745695e-310, 6.93745695e-310, 6.93745695e-310,\n",
       "       6.93745695e-310, 6.93745694e-310, 6.93745694e-310, 6.93745694e-310,\n",
       "       6.93745694e-310, 6.93745694e-310, 6.93745694e-310, 6.93745694e-310,\n",
       "       6.93745694e-310, 6.93745694e-310, 6.93745694e-310, 6.93745694e-310,\n",
       "       6.93745694e-310, 6.93745695e-310, 6.93745695e-310, 6.93745695e-310,\n",
       "       6.93745695e-310, 6.93745695e-310, 6.93745695e-310, 6.93745694e-310,\n",
       "       6.93745694e-310, 6.93745694e-310, 6.93745694e-310, 6.93745694e-310,\n",
       "       6.93745695e-310, 6.93745695e-310, 6.93745695e-310, 6.93745695e-310,\n",
       "       6.93745694e-310, 6.93745694e-310, 0.00000000e+000, 0.00000000e+000,\n",
       "       0.00000000e+000, 6.93744084e-310, 6.90752070e-310, 6.93745694e-310,\n",
       "       6.93745694e-310, 6.93745694e-310, 6.93745694e-310, 6.93745694e-310,\n",
       "       6.93745694e-310, 6.93745694e-310, 6.93745694e-310, 6.93745694e-310,\n",
       "       6.93745695e-310, 6.93745695e-310, 6.93745695e-310, 6.93745695e-310,\n",
       "       6.93745695e-310, 6.93745695e-310, 6.93745695e-310, 6.93745695e-310,\n",
       "       6.93745695e-310, 6.93745695e-310, 6.93745695e-310, 6.93745695e-310,\n",
       "       6.93745695e-310, 6.93745695e-310, 6.93745695e-310, 6.52694311e-314,\n",
       "       6.93745693e-310, 6.93745694e-310, 6.93745694e-310, 6.93745693e-310,\n",
       "       6.93745693e-310, 6.93745693e-310, 6.93745693e-310, 6.93745694e-310,\n",
       "       6.93745693e-310, 6.93745693e-310, 6.93745694e-310, 6.93745695e-310,\n",
       "       6.93745695e-310, 6.93745695e-310, 6.93745695e-310, 6.93745695e-310,\n",
       "       6.93745695e-310, 6.93745695e-310, 6.93745695e-310, 6.93745694e-310,\n",
       "       6.93745694e-310, 6.93745694e-310, 6.93745694e-310, 6.93745694e-310,\n",
       "       6.93745694e-310, 6.93745694e-310, 6.93745694e-310, 6.93745694e-310,\n",
       "       6.93745695e-310, 6.93745695e-310, 6.93745695e-310, 6.93745695e-310,\n",
       "       6.93745695e-310, 6.93745695e-310, 6.93745695e-310, 6.93745695e-310,\n",
       "       6.93745694e-310, 6.93745694e-310, 6.93745694e-310, 6.93745694e-310,\n",
       "       6.93745694e-310, 6.93745694e-310, 6.93745694e-310, 6.93745694e-310,\n",
       "       6.93745694e-310, 6.93745694e-310, 6.93745694e-310, 6.93745694e-310,\n",
       "       6.93745694e-310, 6.93745694e-310, 6.93745694e-310, 6.93745694e-310,\n",
       "       6.93745694e-310, 6.93745695e-310, 6.93745695e-310, 6.93745695e-310,\n",
       "       6.93745695e-310, 6.93745695e-310, 6.93745695e-310, 6.93745695e-310,\n",
       "       6.93745695e-310, 6.93745695e-310, 6.93745695e-310, 6.93745695e-310,\n",
       "       6.93745695e-310, 6.93745695e-310, 6.93745695e-310, 6.93745695e-310,\n",
       "       6.93745695e-310, 6.93745695e-310, 6.93745695e-310, 6.93745694e-310,\n",
       "       6.93745694e-310, 6.93745694e-310, 6.93745694e-310, 6.93745694e-310,\n",
       "       6.93745694e-310, 6.93745694e-310, 6.93745694e-310, 6.93745694e-310,\n",
       "       6.93745694e-310, 6.93745694e-310, 6.93745694e-310, 6.93745694e-310,\n",
       "       6.93745694e-310, 6.93745694e-310, 6.93745694e-310, 6.93745694e-310,\n",
       "       6.93745694e-310, 6.93745694e-310, 6.93745694e-310, 6.93745694e-310,\n",
       "       6.93745694e-310, 6.93745694e-310, 6.93745694e-310, 6.93745694e-310,\n",
       "       6.93745694e-310, 6.93745694e-310, 6.93745694e-310, 6.93745694e-310,\n",
       "       6.93745691e-310, 6.93744084e-310, 6.93745694e-310, 6.93745694e-310,\n",
       "       6.93745694e-310, 6.93745694e-310, 6.93745694e-310, 6.93745694e-310,\n",
       "       6.93745694e-310, 6.93745694e-310, 6.93745693e-310, 6.93745693e-310,\n",
       "       6.93745693e-310, 6.93745693e-310, 6.93745693e-310, 6.93745693e-310,\n",
       "       6.93745693e-310, 6.93745693e-310, 6.93745693e-310, 6.93745693e-310,\n",
       "       6.93745693e-310, 6.93745693e-310, 6.93745693e-310, 6.93745693e-310,\n",
       "       6.93745693e-310, 6.93745694e-310, 6.93745694e-310, 6.93745694e-310,\n",
       "       6.93745694e-310, 6.93745694e-310, 6.93745694e-310, 6.93745693e-310,\n",
       "       6.93745693e-310, 6.93745693e-310, 6.93745693e-310, 6.93745693e-310,\n",
       "       6.93745693e-310, 6.93745693e-310, 6.93745693e-310, 6.93745693e-310,\n",
       "       6.93745693e-310, 6.93745693e-310, 6.93745693e-310, 6.93745693e-310,\n",
       "       6.93745693e-310, 6.93745693e-310, 6.93745693e-310, 6.93745693e-310,\n",
       "       6.93745693e-310, 6.93745693e-310, 6.93745694e-310, 4.94065646e-324,\n",
       "       6.93744084e-310, 0.00000000e+000, 6.93744084e-310, 6.93745691e-310,\n",
       "       6.93745694e-310, 0.00000000e+000, 0.00000000e+000, 0.00000000e+000,\n",
       "       6.93745691e-310, 6.93745691e-310, 0.00000000e+000, 0.00000000e+000,\n",
       "       6.93744084e-310, 6.93745694e-310, 6.93745694e-310, 6.93745694e-310,\n",
       "       0.00000000e+000, 6.93745694e-310, 0.00000000e+000, 6.93745694e-310,\n",
       "       6.93745694e-310, 6.93745694e-310, 6.93745694e-310, 6.93745694e-310,\n",
       "       6.93745694e-310, 6.93745694e-310, 6.93745694e-310, 6.93745694e-310,\n",
       "       6.93745694e-310, 6.93745694e-310, 6.93745695e-310, 6.93745695e-310,\n",
       "       6.93745695e-310, 6.93745695e-310, 6.93745695e-310, 6.93745695e-310,\n",
       "       6.93745695e-310, 6.93745695e-310, 6.93745695e-310, 6.93745694e-310,\n",
       "       6.93745694e-310, 6.93745694e-310, 6.93745694e-310, 6.93745694e-310,\n",
       "       6.93745694e-310, 6.93745694e-310, 6.93745694e-310, 6.93745694e-310,\n",
       "       6.93745694e-310, 6.93745694e-310, 6.93745694e-310, 6.93745694e-310,\n",
       "       6.93745694e-310, 6.93745694e-310, 6.93745694e-310, 6.93745694e-310,\n",
       "       6.93745694e-310, 6.93745694e-310, 6.93745694e-310, 6.93745694e-310,\n",
       "       6.93745694e-310, 6.93745694e-310, 6.93745694e-310, 6.93745694e-310,\n",
       "       6.93745694e-310, 6.93745694e-310, 6.93745694e-310, 6.93745694e-310,\n",
       "       6.93745694e-310, 6.93745694e-310, 6.93745694e-310, 6.93745694e-310,\n",
       "       6.93745694e-310, 6.93745694e-310, 6.93745694e-310, 6.93745694e-310,\n",
       "       6.93745695e-310, 6.93745695e-310, 6.93745695e-310, 6.93745695e-310,\n",
       "       6.93745695e-310, 6.93745695e-310, 6.93745695e-310, 6.93745695e-310,\n",
       "       6.93745695e-310, 6.93745695e-310, 6.93745695e-310, 6.93745695e-310,\n",
       "       6.93745695e-310, 6.93745695e-310, 6.93745695e-310, 6.93745695e-310,\n",
       "       6.93745694e-310, 6.93745694e-310, 6.93745694e-310, 6.93745694e-310,\n",
       "       6.93745694e-310, 6.93745694e-310, 6.93745694e-310, 6.93745694e-310,\n",
       "       6.93745694e-310, 6.93745694e-310, 6.93745694e-310, 6.93745694e-310,\n",
       "       6.93745694e-310, 6.93745694e-310, 6.93745694e-310, 6.93745694e-310,\n",
       "       6.93745694e-310, 6.93745694e-310, 6.93745694e-310, 6.93745694e-310,\n",
       "       6.93745695e-310, 6.93745695e-310, 6.93745695e-310, 6.93745695e-310,\n",
       "       6.93745694e-310, 6.93745694e-310, 6.93745694e-310, 6.93745694e-310,\n",
       "       6.93745694e-310, 6.93745694e-310, 6.93745694e-310, 6.93745694e-310,\n",
       "       6.93745694e-310, 6.93745694e-310, 6.93745694e-310, 6.93745694e-310,\n",
       "       6.93745694e-310, 6.93745694e-310, 6.93745694e-310, 6.93745694e-310,\n",
       "       6.93745694e-310, 6.93745694e-310, 6.93745694e-310, 6.93745694e-310,\n",
       "       6.93745694e-310, 6.93745694e-310, 6.93745694e-310, 6.93745694e-310,\n",
       "       6.93745694e-310, 6.93745694e-310, 6.93745694e-310, 6.93745695e-310,\n",
       "       6.93745695e-310, 6.93745695e-310, 6.93745695e-310, 6.93745695e-310,\n",
       "       6.93745695e-310, 6.93745695e-310, 6.93745695e-310, 6.93745695e-310,\n",
       "       6.93745693e-310, 6.93745693e-310, 6.93745693e-310, 6.93745693e-310,\n",
       "       6.93745693e-310, 6.93745693e-310, 6.93745693e-310, 6.93745693e-310,\n",
       "       6.93745693e-310, 6.93745693e-310, 6.93745693e-310, 6.93745693e-310,\n",
       "       6.93745695e-310, 6.93745695e-310, 6.93745695e-310, 6.93745695e-310,\n",
       "       6.93745695e-310, 6.93745695e-310, 6.93745695e-310, 6.93745695e-310,\n",
       "       6.93745695e-310, 6.93745695e-310, 6.93745695e-310, 6.93745695e-310,\n",
       "       6.93745695e-310, 6.93745695e-310, 6.93745695e-310, 6.93745695e-310,\n",
       "       6.93745694e-310, 6.93745694e-310, 6.93745694e-310, 6.93745694e-310,\n",
       "       6.93745694e-310, 6.93745694e-310, 6.93745694e-310, 6.93745695e-310,\n",
       "       6.93745695e-310, 6.93745695e-310, 6.93745695e-310, 6.93745695e-310,\n",
       "       6.93745695e-310, 6.93745695e-310, 6.93745695e-310, 6.93745695e-310,\n",
       "       6.93745695e-310, 6.93745695e-310, 6.93745695e-310, 6.93745695e-310,\n",
       "       6.93745695e-310, 6.93745695e-310, 6.93745695e-310, 6.93745695e-310,\n",
       "       6.93745695e-310, 6.93745695e-310, 6.93745695e-310, 6.93745695e-310,\n",
       "       6.93745694e-310, 6.93745695e-310, 6.93745695e-310, 6.93745695e-310,\n",
       "       6.93745695e-310, 6.93745693e-310, 6.93745693e-310, 6.93745694e-310,\n",
       "       6.93745693e-310, 6.93745693e-310, 6.93745693e-310, 6.93745693e-310,\n",
       "       6.93745693e-310, 6.93745695e-310, 6.93745694e-310, 6.93745694e-310,\n",
       "       6.93745694e-310, 6.93745695e-310, 6.93745695e-310, 6.93745694e-310,\n",
       "       6.93745695e-310, 6.93745695e-310, 6.93745694e-310, 6.93745694e-310,\n",
       "       6.93745694e-310, 6.93745694e-310, 6.93745695e-310, 6.93745694e-310,\n",
       "       6.93745694e-310, 6.93745694e-310, 6.93745694e-310, 6.93745694e-310,\n",
       "       6.93745694e-310, 6.93745695e-310, 6.93745695e-310, 6.93745695e-310,\n",
       "       6.93745695e-310, 6.93745694e-310, 6.93745695e-310, 6.93745695e-310,\n",
       "       6.93745695e-310, 6.93745695e-310, 6.93745695e-310, 6.93745695e-310,\n",
       "       6.93745695e-310, 6.93745694e-310, 6.93745694e-310, 6.93745694e-310,\n",
       "       6.93745694e-310, 6.93745695e-310, 6.93745695e-310, 6.93745695e-310])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numdocs, numwords = np.shape(bag_ow)\n",
    "N = numdocs\n",
    "word_frequency = np.empty(numwords)\n",
    "word_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d890849c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3d38b494",
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in range(numwords):\n",
    "    word_frequency[word] = np.sum((bag_ow[:,word]>0))\n",
    "idf = np.log(N/word_frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b421f1b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500,)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d57bcb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initializs tfidf array\n",
    "tfidf = np.empty([numdocs, numwords])\n",
    "\n",
    "#loop through the tweets, multiply term frequency (represented by bag of words) with idf\n",
    "for doc in range(numdocs):\n",
    "    tfidf[doc, :]=bag_ow[doc, :]*idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1de3d1b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2712, 500)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6e60c765",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 3.23888711, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.9917043 , 0.        , 1.61944355, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 1.61944355, ..., 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "65968ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression #to import logistic regression model\n",
    "from sklearn.model_selection import train_test_split #to split data into training and testing set\n",
    "from sklearn.model_selection import GridSearchCV #to find out the best parameter for our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e8ecdcf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split X_all and y_all into training and testing sets\n",
    "X_train,X_test,y_train,y_test = train_test_split(tfidf,df['sentiment'].values,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "da02869e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2034, 500)\n",
      "(678, 500)\n",
      "(2034,)\n",
      "(678,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "99be2f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model instance\n",
    "logreg = LogisticRegression(solver = 'lbfgs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f0f36f09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fit the model on the training set\n",
    "logreg.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "07b964fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['negative' 'positive' 'positive' 'negative' 'negative' 'positive'\n",
      " 'positive' 'negative' 'positive' 'positive' 'negative' 'positive'\n",
      " 'positive' 'negative' 'negative' 'positive' 'positive' 'positive'\n",
      " 'positive' 'negative' 'positive' 'negative' 'positive' 'positive'\n",
      " 'positive' 'positive' 'positive' 'positive' 'positive' 'negative'\n",
      " 'negative' 'positive' 'positive' 'positive' 'positive' 'positive'\n",
      " 'negative' 'positive' 'positive' 'positive' 'positive' 'positive'\n",
      " 'negative' 'positive' 'positive' 'negative' 'negative' 'positive'\n",
      " 'negative' 'positive' 'positive' 'negative' 'negative' 'positive'\n",
      " 'positive' 'positive' 'positive' 'negative' 'positive' 'negative'\n",
      " 'positive' 'positive' 'positive' 'positive' 'positive' 'positive'\n",
      " 'positive' 'positive' 'positive' 'positive' 'negative' 'positive'\n",
      " 'negative' 'positive' 'negative' 'negative' 'negative' 'negative'\n",
      " 'negative' 'negative' 'negative' 'negative' 'positive' 'positive'\n",
      " 'negative' 'positive' 'positive' 'positive' 'positive' 'positive'\n",
      " 'positive' 'positive' 'negative' 'positive' 'positive' 'positive'\n",
      " 'negative' 'positive' 'positive' 'positive' 'positive' 'positive'\n",
      " 'positive' 'negative' 'positive' 'positive' 'positive' 'positive'\n",
      " 'positive' 'positive' 'positive' 'positive' 'positive' 'positive'\n",
      " 'negative' 'positive' 'positive' 'positive' 'positive' 'positive'\n",
      " 'negative' 'positive' 'positive' 'positive' 'positive' 'positive'\n",
      " 'positive' 'positive' 'positive' 'positive' 'positive' 'negative'\n",
      " 'positive' 'positive' 'positive' 'negative' 'positive' 'negative'\n",
      " 'positive' 'positive' 'negative' 'positive' 'positive' 'negative'\n",
      " 'negative' 'positive' 'positive' 'positive' 'positive' 'positive'\n",
      " 'negative' 'negative' 'negative' 'positive' 'positive' 'negative'\n",
      " 'positive' 'negative' 'negative' 'positive' 'positive' 'negative'\n",
      " 'positive' 'positive' 'positive' 'positive' 'positive' 'positive'\n",
      " 'positive' 'positive' 'positive' 'negative' 'positive' 'negative'\n",
      " 'negative' 'positive' 'negative' 'positive' 'negative' 'positive'\n",
      " 'negative' 'positive' 'negative' 'positive' 'positive' 'positive'\n",
      " 'positive' 'positive' 'positive' 'positive' 'positive' 'negative'\n",
      " 'positive' 'positive' 'negative' 'negative' 'negative' 'positive'\n",
      " 'negative' 'positive' 'negative' 'positive' 'positive' 'positive'\n",
      " 'positive' 'positive' 'positive' 'positive' 'negative' 'negative'\n",
      " 'negative' 'positive' 'negative' 'positive' 'positive' 'negative'\n",
      " 'positive' 'positive' 'positive' 'positive' 'negative' 'negative'\n",
      " 'negative' 'positive' 'positive' 'negative' 'positive' 'negative'\n",
      " 'negative' 'negative' 'negative' 'positive' 'negative' 'positive'\n",
      " 'positive' 'positive' 'positive' 'positive' 'positive' 'positive'\n",
      " 'positive' 'negative' 'positive' 'positive' 'positive' 'negative'\n",
      " 'positive' 'negative' 'negative' 'positive' 'positive' 'positive'\n",
      " 'positive' 'negative' 'positive' 'positive' 'positive' 'positive'\n",
      " 'positive' 'positive' 'positive' 'positive' 'positive' 'positive'\n",
      " 'positive' 'negative' 'positive' 'negative' 'positive' 'positive'\n",
      " 'positive' 'positive' 'positive' 'positive' 'positive' 'positive'\n",
      " 'positive' 'positive' 'negative' 'negative' 'positive' 'positive'\n",
      " 'positive' 'negative' 'positive' 'positive' 'negative' 'positive'\n",
      " 'negative' 'positive' 'positive' 'positive' 'negative' 'positive'\n",
      " 'negative' 'positive' 'positive' 'positive' 'negative' 'positive'\n",
      " 'positive' 'positive' 'positive' 'positive' 'negative' 'positive'\n",
      " 'positive' 'positive' 'positive' 'positive' 'positive' 'positive'\n",
      " 'negative' 'positive' 'positive' 'positive' 'positive' 'positive'\n",
      " 'positive' 'negative' 'positive' 'negative' 'positive' 'positive'\n",
      " 'negative' 'negative' 'negative' 'positive' 'positive' 'negative'\n",
      " 'positive' 'positive' 'negative' 'negative' 'positive' 'negative'\n",
      " 'positive' 'negative' 'negative' 'negative' 'negative' 'positive'\n",
      " 'positive' 'positive' 'negative' 'negative' 'positive' 'negative'\n",
      " 'positive' 'negative' 'positive' 'positive' 'positive' 'positive'\n",
      " 'positive' 'positive' 'negative' 'negative' 'positive' 'negative'\n",
      " 'positive' 'positive' 'positive' 'negative' 'negative' 'positive'\n",
      " 'negative' 'negative' 'positive' 'positive' 'positive' 'positive'\n",
      " 'positive' 'negative' 'positive' 'positive' 'negative' 'positive'\n",
      " 'negative' 'negative' 'positive' 'negative' 'positive' 'negative'\n",
      " 'positive' 'positive' 'negative' 'positive' 'positive' 'positive'\n",
      " 'positive' 'negative' 'positive' 'positive' 'negative' 'positive'\n",
      " 'positive' 'negative' 'positive' 'positive' 'positive' 'positive'\n",
      " 'positive' 'negative' 'negative' 'positive' 'negative' 'negative'\n",
      " 'negative' 'positive' 'positive' 'positive' 'positive' 'negative'\n",
      " 'positive' 'positive' 'positive' 'positive' 'negative' 'negative'\n",
      " 'positive' 'negative' 'positive' 'positive' 'positive' 'positive'\n",
      " 'positive' 'positive' 'positive' 'negative' 'positive' 'positive'\n",
      " 'positive' 'positive' 'negative' 'positive' 'positive' 'positive'\n",
      " 'positive' 'positive' 'positive' 'negative' 'negative' 'positive'\n",
      " 'negative' 'positive' 'positive' 'positive' 'positive' 'positive'\n",
      " 'positive' 'positive' 'positive' 'positive' 'positive' 'negative'\n",
      " 'positive' 'negative' 'positive' 'negative' 'positive' 'negative'\n",
      " 'positive' 'positive' 'positive' 'negative' 'negative' 'positive'\n",
      " 'positive' 'positive' 'positive' 'negative' 'positive' 'positive'\n",
      " 'negative' 'negative' 'positive' 'positive' 'negative' 'positive'\n",
      " 'positive' 'negative' 'positive' 'positive' 'positive' 'positive'\n",
      " 'negative' 'positive' 'positive' 'positive' 'negative' 'positive'\n",
      " 'negative' 'positive' 'positive' 'positive' 'negative' 'positive'\n",
      " 'positive' 'positive' 'positive' 'positive' 'positive' 'negative'\n",
      " 'positive' 'negative' 'negative' 'positive' 'negative' 'positive'\n",
      " 'positive' 'negative' 'positive' 'positive' 'negative' 'positive'\n",
      " 'negative' 'positive' 'positive' 'positive' 'positive' 'negative'\n",
      " 'negative' 'negative' 'negative' 'positive' 'positive' 'negative'\n",
      " 'negative' 'positive' 'positive' 'positive' 'negative' 'positive'\n",
      " 'negative' 'negative' 'positive' 'positive' 'positive' 'positive'\n",
      " 'positive' 'positive' 'positive' 'negative' 'positive' 'negative'\n",
      " 'negative' 'negative' 'positive' 'positive' 'negative' 'positive'\n",
      " 'negative' 'positive' 'positive' 'positive' 'positive' 'positive'\n",
      " 'positive' 'negative' 'positive' 'positive' 'positive' 'negative'\n",
      " 'negative' 'positive' 'negative' 'positive' 'negative' 'positive'\n",
      " 'negative' 'negative' 'positive' 'negative' 'positive' 'positive'\n",
      " 'negative' 'positive' 'positive' 'positive' 'negative' 'negative'\n",
      " 'negative' 'positive' 'positive' 'positive' 'positive' 'positive'\n",
      " 'negative' 'negative' 'negative' 'positive' 'negative' 'negative'\n",
      " 'positive' 'positive' 'positive' 'negative' 'positive' 'negative'\n",
      " 'positive' 'negative' 'negative' 'positive' 'negative' 'positive'\n",
      " 'positive' 'positive' 'negative' 'positive' 'negative' 'positive'\n",
      " 'positive' 'negative' 'positive' 'positive' 'negative' 'positive'\n",
      " 'positive' 'negative' 'negative' 'positive' 'negative' 'positive'\n",
      " 'negative' 'positive' 'positive' 'negative' 'negative' 'positive'\n",
      " 'positive' 'negative' 'positive' 'positive' 'positive' 'positive'\n",
      " 'negative' 'positive' 'negative' 'positive' 'positive' 'positive'\n",
      " 'positive' 'positive' 'positive' 'negative' 'positive' 'negative'\n",
      " 'positive' 'negative' 'positive' 'positive' 'negative' 'positive'\n",
      " 'positive' 'positive' 'positive' 'positive' 'positive' 'negative'\n",
      " 'positive' 'positive' 'negative' 'positive' 'negative' 'positive'\n",
      " 'negative' 'negative' 'positive' 'positive' 'positive' 'positive'\n",
      " 'positive' 'positive' 'positive' 'negative' 'positive' 'positive']\n"
     ]
    }
   ],
   "source": [
    "y_pred=logreg.predict(X_test)\n",
    "print (y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f6079dd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7905604719764012\n"
     ]
    }
   ],
   "source": [
    "score = logreg.score(X_test, y_test)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "74453500",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_predictor(news):\n",
    "    # your code here\n",
    "    word_vector = bagofwords(news, vocab) # set a variable with bag of words. Remember the bagofwords function you have created?\n",
    "    word_tfidf = word_vector*idf #find tfidf value\n",
    "    prediction = logreg.predict(word_tfidf.reshape(1, -1)) #predict wether news is positive or not\n",
    "    result = {'positive' :1, 'negative' :2}\n",
    "    #results = {1:'positive', 0:'negative'} #creating a set containing the potential results. \n",
    "    #print(result[int(prediction)])\n",
    "    print(str(prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d19ae34a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['positive']\n"
     ]
    }
   ],
   "source": [
    "sentence_1 = \"Operating profit fell to USD 38.1 mn from USD 40\" #negative\n",
    "sentence_2 = \"ABC wouldn't be surprised to see a green close\" #positive\n",
    "sentiment_predictor(sentence_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4d6d75b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The GeoSolutions technology will leverage Bene...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>$ESI on lows, down $1.50 to $2.50 BK a real po...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>For the last quarter of 2010 , Componenta 's n...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>$SPY wouldn't be surprised to see a green close</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Shell's $70 Billion BG Deal Meets Shareholder ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5832</th>\n",
       "      <td>Operating profit fell to EUR 38.1 mn from EUR ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5835</th>\n",
       "      <td>HSBC Says Unit to Book $585 Million Charge on ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5836</th>\n",
       "      <td>Daily Mail parent company in talks with potent...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5837</th>\n",
       "      <td>RISING costs have forced packaging producer Hu...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5841</th>\n",
       "      <td>HELSINKI AFX - KCI Konecranes said it has won ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2712 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sentence sentiment  value\n",
       "0     The GeoSolutions technology will leverage Bene...  positive      1\n",
       "1     $ESI on lows, down $1.50 to $2.50 BK a real po...  negative      0\n",
       "2     For the last quarter of 2010 , Componenta 's n...  positive      1\n",
       "5       $SPY wouldn't be surprised to see a green close  positive      1\n",
       "6     Shell's $70 Billion BG Deal Meets Shareholder ...  negative      0\n",
       "...                                                 ...       ...    ...\n",
       "5832  Operating profit fell to EUR 38.1 mn from EUR ...  negative      0\n",
       "5835  HSBC Says Unit to Book $585 Million Charge on ...  negative      0\n",
       "5836  Daily Mail parent company in talks with potent...  positive      1\n",
       "5837  RISING costs have forced packaging producer Hu...  negative      0\n",
       "5841  HELSINKI AFX - KCI Konecranes said it has won ...  positive      1\n",
       "\n",
       "[2712 rows x 3 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "595d8054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the news ?stocks fell by 33%\n",
      "['negative']\n"
     ]
    }
   ],
   "source": [
    "sentence = input('What is the news ?')\n",
    "sentiment_predictor(sentence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
